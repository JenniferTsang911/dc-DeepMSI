{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jenni\\anaconda3\\envs\\DESIproject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(input_file_path, file_name):\n",
    "    \n",
    "    file_path = os.path.join(input_file_path, f'{file_name}.csv')\n",
    "\n",
    "    # Load the CSV dataset into a Pandas DataFrame\n",
    "    df = pd.read_csv(file_path, header=0)\n",
    "\n",
    "    # Ignore the first column of the DataFrame\n",
    "    df = df.iloc[:, 1:]\n",
    "\n",
    "    # Find unique classes in the second column\n",
    "    unique_classes = df.iloc[:, 0].unique()\n",
    "\n",
    "    # Create a dictionary to store the data frames for each class\n",
    "    class_dataframes = {class_name: df[df.iloc[:, 0] == class_name] for class_name in unique_classes}\n",
    "\n",
    "    # Find the minimum number of samples among the classes\n",
    "    min_samples = min(len(class_df) for class_df in class_dataframes.values())\n",
    "\n",
    "    # Randomly select and remove rows from classes with more samples\n",
    "    for class_name, class_df in class_dataframes.items():\n",
    "        if len(class_df) > min_samples:\n",
    "            class_dataframes[class_name] = class_df.sample(min_samples)\n",
    "\n",
    "    # Concatenate the dataframes for each class back into a single dataframe\n",
    "    balanced_df = pd.concat(class_dataframes.values())\n",
    "\n",
    "    print(balanced_df.iloc[:, 0].value_counts())\n",
    "\n",
    "    output_file_path = os.path.join(input_file_path, f'{file_name}_balanced.csv')\n",
    "\n",
    "    # Save the balanced dataset to a new CSV file\n",
    "    balanced_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adenocarcinoma    457\n",
      "benign mucosa     457\n",
      "smooth muscle     457\n",
      "serosa            457\n",
      "submucosa         457\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "input_file_path = r\"C:\\Users\\jenni\\OneDrive - Queen's University\\DESI project\\DESI TXT colon\\Annotated Dataset\"\n",
    "file_name = \"2021 03 30 colon 0413337-2 Analyte 6_dataset\"\n",
    "\n",
    "balance_dataset(input_file_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train(output, csv_path, patch_size, dim_y, dim_x):\n",
    "    # Extract patches from the output feature maps\n",
    "    output_patches = output.unfold(1, patch_size, 1).unfold(2, patch_size, 1)\n",
    "    print(output_patches.size())\n",
    "\n",
    "    # Flatten the patches\n",
    "    output_patches_flattened = output_patches.reshape(dim_y, dim_x, -1)\n",
    "    print(output_patches_flattened.shape)\n",
    "\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    unique_classes = df['Class'].unique()\n",
    "    print(f\"Unique classes: {unique_classes}\")\n",
    "\n",
    "    # Create a dictionary where the keys are the (X, Y) coordinates and the values are the class labels\n",
    "    csv_dict = {(row['X'], row['Y']): row for _, row in df.iterrows()}\n",
    "\n",
    "    # Initialize the features and labels\n",
    "    labels = []\n",
    "    features = []\n",
    "    mz_values = []\n",
    "\n",
    "    # For each patch\n",
    "    for y in range(0, output_patches_flattened.shape[0] - patch_size + 1):\n",
    "        for x in range(0, output_patches_flattened.shape[1] - patch_size + 1):\n",
    "            \n",
    "            # Get the class labels and m/z values for the corresponding region in the CSV file\n",
    "            patch_labels = [csv_dict.get((x + dx, y + dy))['Class'] for dx in range(patch_size) for dy in range(patch_size) if csv_dict.get((x + dx, y + dy)) is not None]\n",
    "            # Get the m/z values for the corresponding region in the CSV file\n",
    "            patch_mz_values = [csv_dict.get((x + dx, y + dy))[3:] for dx in range(patch_size) for dy in range(patch_size) if csv_dict.get((x + dx, y + dy)) is not None]\n",
    "\n",
    "            # If there are any class labels for the corresponding region in the CSV file\n",
    "            if patch_labels:\n",
    "                # Get the most common class label in the region\n",
    "                most_common_label = stats.mode(patch_labels)[0][0]\n",
    "\n",
    "                # Append the output patch to the features list\n",
    "                features.append(output_patches_flattened[y, x, :])\n",
    "\n",
    "                # Append the most common class label\n",
    "                labels.append(most_common_label)\n",
    "\n",
    "                # Append the mean m/z value for the patch\n",
    "                mz_values.append(np.mean(patch_mz_values))\n",
    "\n",
    "    # Convert the lists to NumPy arrays\n",
    "    labels = np.array(labels)\n",
    "    features = np.array(features).reshape(-1, 1)  # Reshape features to be a 2D array\n",
    "    mz_values = np.array(mz_values).reshape(-1, 1)\n",
    "\n",
    "    # Stack the feature arrays horizontally\n",
    "    features = np.hstack((features, mz_values.reshape(-1, 1)))\n",
    "\n",
    "    # Handle NaN values\n",
    "    features = np.nan_to_num(features, nan=0.0)\n",
    "\n",
    "    # Encode the class labels as integers\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "\n",
    "    for i, class_label in enumerate(le.classes_):\n",
    "        print(f\"{i}: {class_label}\")\n",
    "\n",
    "    # Split the features and labels into a training set and a test set\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    return features_train, features_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_test_train(output, csv_path, patch_size):\n",
    "#     # Extract patches from the output feature maps\n",
    "#     output_patches = output.unfold(1, patch_size, 1).unfold(2, patch_size, 1)\n",
    "#     print(output_patches.size())\n",
    "\n",
    "#     # Flatten the patches\n",
    "#     output_patches_flattened = output_patches.reshape(output_patches.shape[0], -1)\n",
    "#     print(output_patches_flattened.shape)\n",
    "\n",
    "#     # Load the CSV file\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     df.fillna(0, inplace=True)\n",
    "\n",
    "#     unique_classes = df['Class'].unique()\n",
    "#     print(f\"Unique classes: {unique_classes}\")\n",
    "\n",
    "#     # Create a dictionary where the keys are the (X, Y) coordinates and the values are the class labels\n",
    "#     csv_dict = {(row['X'], row['Y']): row for _, row in df.iterrows()}\n",
    "\n",
    "#     # Initialize the features and labels\n",
    "#     features = []\n",
    "#     labels = []\n",
    "\n",
    "#     print(output.shape)\n",
    "\n",
    "#     #205 263\n",
    "\n",
    "#     # For each patch\n",
    "#     for y in range(0, output.shape[0] - patch_size + 1):\n",
    "#         for x in range(0, output.shape[1] - patch_size + 1):\n",
    "#             # Get the class labels for the corresponding region in the CSV file\n",
    "#             patch_labels = [csv_dict.get((x + dx, y + dy))['Class'] for dx in range(patch_size) for dy in range(patch_size) if csv_dict.get((x + dx, y + dy)) is not None]\n",
    "\n",
    "#             patch = output[x:x+patch_size, y:y+patch_size]\n",
    "\n",
    "#             # Print the patch\n",
    "#             #print(f\"Patch at ({x}, {y}):\")\n",
    "#             #print(patch)\n",
    "\n",
    "#             # If there are any class labels for the corresponding region in the CSV file\n",
    "#             if patch_labels:\n",
    "#                 # Get the most common class label in the region\n",
    "#                 most_common_label = stats.mode(patch_labels)[0][0]\n",
    "\n",
    "#                 # features.append(np.concatenate([output_patches_flattened[y * output.shape[1] + x, :], df.loc[\n",
    "#                 #                                     (df['Y'] == x) & (df['X'] == y), df.columns[3:]].mean().values]))\n",
    "\n",
    "#                 features.append(np.concatenate([output_patches_flattened[y * output.shape[1] + x, :], df.loc[\n",
    "#                                                     (df['X'] == x) & (df['Y'] == y), df.columns[3:]].mean().values]))\n",
    "\n",
    "#                 print(f\"Most common class label: {most_common_label}\")\n",
    "\n",
    "#                 # Add the most common class label to the labels\n",
    "#                 labels.append(most_common_label)\n",
    "\n",
    "#     unique_labels = np.unique(labels)\n",
    "#     print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "#     # Handle NaN values\n",
    "#     features = np.nan_to_num(features, nan=0.0)\n",
    "\n",
    "#     # Encode the class labels as integers\n",
    "#     le = LabelEncoder()\n",
    "#     labels = le.fit_transform(labels)\n",
    "\n",
    "#     for i, class_label in enumerate(le.classes_):\n",
    "#         print(f\"{i}: {class_label}\")\n",
    "\n",
    "#     # Split the features and labels into a training set and a test set\n",
    "#     features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2,\n",
    "#                                                                                 random_state=42)\n",
    "#     return features_train, features_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([53915, 56, 1, 5])\n",
      "torch.Size([205, 263, 280])\n",
      "Unique classes: ['adenocarcinoma' 'benign mucosa' 'smooth muscle' 'serosa' 'submucosa']\n",
      "0: adenocarcinoma\n",
      "1: benign mucosa\n",
      "2: serosa\n",
      "3: smooth muscle\n",
      "4: submucosa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jenni\\anaconda3\\envs\\DESIproject\\lib\\site-packages\\ipykernel_launcher.py:52: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "c:\\Users\\jenni\\anaconda3\\envs\\DESIproject\\lib\\site-packages\\ipykernel_launcher.py:52: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "csv_path = r\"C:\\Users\\jenni\\OneDrive - Queen's University\\DESI project\\DESI TXT colon\\Annotated Dataset\\2021 03 30 colon 0413337-2 Analyte 6_dataset_balanced.csv\"\n",
    "patch_size = 5\n",
    "\n",
    "output = np.load(r\"C:\\Users\\jenni\\OneDrive - Queen's University\\DESI project\\DESI TXT colon\\dc-DeepMSI outputs\\w5 nc60\\2021 03 30 colon 0413337-2 Analyte 6 array.npy\")\n",
    "\n",
    "output = torch.from_numpy(output)\n",
    "#torch.Size([51980, 30]) torch.Size([51980, 30])\n",
    "\n",
    "#205 263\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = get_test_train(output, csv_path, patch_size, 205, 263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(features_train, features_test, labels_train, labels_test):\n",
    "    # Train the logistic regression model\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features_train = scaler.fit_transform(features_train)\n",
    "    features_test = scaler.transform(features_test)\n",
    "\n",
    "    print(\"Training\")\n",
    "    clf = LogisticRegression(max_iter=1000, solver= 'newton-cg')\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "\n",
    "    print(\"Predicting\")\n",
    "    predicted_labels = clf.predict(features_test)\n",
    "\n",
    "    #Accuracy\n",
    "    accuracy = accuracy_score(labels_test, predicted_labels)\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "\n",
    "    # Classification report\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(labels_test, predicted_labels))\n",
    "\n",
    "    # Confustion matrix\n",
    "    c_matrix = confusion_matrix(labels_test, predicted_labels)\n",
    "    print(\"Confusion Matrix: \" + str(c_matrix))\n",
    "\n",
    "    c_matrix = confusion_matrix(labels_test, predicted_labels)\n",
    "    sns.heatmap(c_matrix, annot=True, fmt='d')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression_grid(features_train, features_test, labels_train, labels_test):\n",
    "    # Existing code...\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5)\n",
    "\n",
    "    # Fit the GridSearchCV object to the data\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "\n",
    "    # Print the best parameters\n",
    "    print(\"Best Parameters: \", grid_search.best_params_)\n",
    "\n",
    "    # Use the best model to make predictions\n",
    "    clf = grid_search.best_estimator_\n",
    "    predicted_labels = clf.predict(features_test)\n",
    "\n",
    "    accuracy = accuracy_score(labels_test, predicted_labels)\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    c_matrix = confusion_matrix(labels_test, predicted_labels)\n",
    "    print(\"Confusion Matrix: \" + str(c_matrix))\n",
    "\n",
    "    c_matrix = confusion_matrix(labels_test, predicted_labels)\n",
    "    sns.heatmap(c_matrix, annot=True, fmt='d')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegression(features_train, features_test, labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRF(features_train, features_test, labels_train, labels_test):\n",
    "\trf_model = RandomForestClassifier()\n",
    "\trf_model.fit(features_train, labels_train)\n",
    "\ty_test_preds = rf_model.predict(features_test)\n",
    "\n",
    "\taccuracy = accuracy_score(labels_test, y_test_preds)\n",
    "\tprint(\"Accuracy: \" + str(accuracy))\n",
    "\n",
    "\t# Print the confusion matrix\n",
    "\tc_matrix = confusion_matrix(labels_test, y_test_preds)\n",
    "\tprint(\"Confusion Matrix: \")\n",
    "\tprint(c_matrix)\n",
    "\n",
    "\t\t# Print the classification report\n",
    "\tprint(\"Classification Report: \")\n",
    "\tprint(classification_report(labels_test, y_test_preds))\n",
    "\n",
    "\t# Visualize the confusion matrix using a heatmap\n",
    "\tplt.figure(figsize=(10,7))\n",
    "\tsns.heatmap(c_matrix, annot=True, fmt='d')\n",
    "\tplt.xlabel('Predicted')\n",
    "\tplt.ylabel('Truth')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runrf_gridsearch(X_train, y_train):\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Create a base model\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    # Instantiate the grid search model\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runrf_gridsearch(features_train, features_test, labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decisiontree(features_train, features_test, labels_train, labels_test):\n",
    "    # Create a decision tree classifier\n",
    "    clf = DecisionTreeClassifier()\n",
    "\n",
    "    # Train the classifier\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "    # Make predictions\n",
    "    predicted_labels = clf.predict(features_test)\n",
    "\n",
    "    # Print the accuracy\n",
    "    accuracy = accuracy_score(labels_test, predicted_labels)\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    c_matrix = confusion_matrix(labels_test, predicted_labels)\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(c_matrix)\n",
    "\n",
    "    # Print the classification report\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(labels_test, predicted_labels))\n",
    "\n",
    "    # Visualize the confusion matrix using a heatmap\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(c_matrix, annot=True, fmt='d')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Truth')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_decisiontree_gridsearch(features_train, features_test, labels_train, labels_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESIproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
